{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "README\n",
    "\n",
    "本程序要求的包如下：\n",
    "    ipykernel (为了可以在ipynb中运行)\n",
    "    tqdm (为了可视化训练的进度)\n",
    "    gensim (为了导入wiki_word2vec50.bin)\n",
    "    torch (为了使用pytorch框架)\n",
    "请确认可以import这四个库\n",
    "\n",
    "如果只是要验收模型的实验效果，可以直接移步最后标有“验收旧模型”的三个模块\n",
    "如果要调整参数，训练新模型，可以在调整模型参数后，运行有关的函数定义模块，再依次运行标有“训练新模型”的三个模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 947,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型参数\n",
    "\n",
    "# 公共参数\n",
    "sentence_len = 50\n",
    "embedding_size = 50\n",
    "batch_size = 50\n",
    "learning_rate = 0.001\n",
    "max_epoch = 10\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format('Dataset/wiki_word2vec_50.bin', binary=True).vectors\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# TextCNN参数\n",
    "kernel_sizes = [3, 5, 7, 9]\n",
    "kernel_nums = 20\n",
    "\n",
    "# TextRNN参数\n",
    "hidden_size = 100\n",
    "hidden_nums = 2\n",
    "\n",
    "# TextMLP参数\n",
    "fc_size = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理\n",
    "# 生成6个json文件，如果修改了sentence_len参数，需要重新运行本段代码，否则直接使用已生成的6个json文件即可\n",
    "\n",
    "import gensim\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "gensim_model = gensim.models.KeyedVectors.load_word2vec_format('Dataset/wiki_word2vec_50.bin', binary=True)\n",
    "pretrained_word2vec = torch.FloatTensor(gensim_model.vectors)\n",
    "word2index = {word: i for i, word in enumerate(gensim_model.index_to_key)}\n",
    "\n",
    "def get_sentences_and_labels(name):\n",
    "    txt_file = open(\"./Dataset/\"+name+\".txt\")\n",
    "    txt_lines = txt_file.readlines()\n",
    "    txt_file.close()\n",
    "\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    for txt_line in txt_lines:\n",
    "        txt_line = txt_line.split()\n",
    "        sentence = []\n",
    "        for word in txt_line[1:]:\n",
    "            index = word2index[word] if word in gensim_model.index_to_key else 0; \n",
    "            sentence.append(index)\n",
    "            if len(sentence) == sentence_len:\n",
    "                break\n",
    "        if len(sentence) < sentence_len:\n",
    "            sentence = sentence + [0 for _ in range(sentence_len - len(sentence))]\n",
    "        sentences.append(sentence)\n",
    "        labels.append(int(txt_line[0]))\n",
    "    return sentences, labels\n",
    "\n",
    "def gen_and_save_sentences_and_labels(name):\n",
    "    (sentences, labels) = get_sentences_and_labels(name)\n",
    "    sentence_file = open(name+\"_sentences.json\", \"w\")\n",
    "    json.dump(sentences, sentence_file)\n",
    "    sentence_file.close()\n",
    "    label_file = open(name+\"_label.json\", \"w\")\n",
    "    json.dump(labels, label_file)\n",
    "    label_file.close()\n",
    "\n",
    "gen_and_save_sentences_and_labels(\"train\")\n",
    "gen_and_save_sentences_and_labels(\"validation\")\n",
    "gen_and_save_sentences_and_labels(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 878,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TextCNN模型定义\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(word2vec))\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(in_channels=1, out_channels=kernel_nums, kernel_size=(kernel_size, embedding_size)) for kernel_size in kernel_sizes])\n",
    "        self.full_connect = nn.Linear(in_features=len(kernel_sizes)*kernel_nums, out_features=2)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        conveds = [F.relu(conv(embedded.unsqueeze(1)).squeeze(3)) for conv in self.convs]\n",
    "        pooled = [F.max_pool1d(conv, conv.size(2)).squeeze(2) for conv in conveds]\n",
    "        cat = torch.cat(pooled, dim=1)\n",
    "        full_connected = self.full_connect(cat)\n",
    "        output = self.softmax(full_connected)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 950,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TextRNN_LSTM模型定义\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TextRNN_LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextRNN_LSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(word2vec))\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers=hidden_nums, bidirectional=True)\n",
    "        self.full_connect = nn.Linear(in_features=2*hidden_size, out_features=2)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        output, (hidden, cell) = self.rnn(embedded.permute(1, 0, 2))\n",
    "        hidden = hidden.view(hidden_nums, 2, -1, hidden_size)\n",
    "        cat = torch.cat((hidden[-1, 0], hidden[-1, 1]), dim=1)\n",
    "        full_connected = self.full_connect(cat)\n",
    "        output = self.softmax(full_connected)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 953,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TextRNN_GRU 模型定义\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TextRNN_GRU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextRNN_GRU, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(word2vec))\n",
    "        self.rnn = nn.GRU(embedding_size, hidden_size, num_layers=hidden_nums, bidirectional=True)\n",
    "        self.full_connect = nn.Linear(in_features=2*hidden_size, out_features=2)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        output, hidden = self.rnn(embedded.permute(1, 0, 2))\n",
    "        hidden = hidden.view(hidden_nums, 2, -1, hidden_size)\n",
    "        cat = torch.cat((hidden[-1, 0], hidden[-1, 1]), dim=1)\n",
    "        full_connected = self.full_connect(cat)\n",
    "        output = self.softmax(full_connected)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 948,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TextMLP模型定义\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TextMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextMLP, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(word2vec))\n",
    "        self.full_connect_1 = nn.Linear(in_features=embedding_size*sentence_len, out_features=fc_size)\n",
    "        self.full_connect_2 = nn.Linear(in_features=fc_size, out_features=2)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        full_connected_1 = F.relu(self.full_connect_1(embedded.view(-1, embedding_size*sentence_len)))\n",
    "        full_connected_2 = F.relu(self.full_connect_2(full_connected_1))\n",
    "        output = self.softmax(full_connected_2)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成DataLoader的函数定义\n",
    "\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class EmoDataset(Dataset):\n",
    "    def __init__(self, sentences, labels):\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return torch.tensor(self.sentences[index]), torch.tensor(self.labels[index])\n",
    "\n",
    "def load_data(name):\n",
    "    sentence_file = open(name+\"_sentences.json\")\n",
    "    sentences = json.load(sentence_file)\n",
    "    sentence_file.close()\n",
    "\n",
    "    label_file = open(name+\"_label.json\")\n",
    "    labels = json.load(label_file)\n",
    "    label_file.close()\n",
    "\n",
    "    dataset = EmoDataset(sentences, labels)\n",
    "    return DataLoader(dataset, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 808,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试与检验的函数定义\n",
    "\n",
    "import torch\n",
    "\n",
    "def evaluate(loader):\n",
    "    P = 0\n",
    "    total = 0\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    T = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            sentences, labels = data\n",
    "            outputs = model.forward(sentences.to(device))\n",
    "            predicts = torch.argmax(outputs, dim=1)\n",
    "            total += len(predicts)\n",
    "            for i in range(len(predicts)):\n",
    "                if predicts[i] == labels[i]:\n",
    "                    T += 1\n",
    "                if predicts[i] == 0:\n",
    "                    if labels[i] == 0:\n",
    "                        TP += 1\n",
    "                    else:\n",
    "                        FP += 1\n",
    "                if labels[i] == 0:\n",
    "                    P += 1\n",
    "    accuracy = float(T) / total * 100\n",
    "    precision = float(TP) / (TP+FP)\n",
    "    recall = float(TP) / P\n",
    "    f1_score = float(2) / (1/precision + 1/recall)\n",
    "    print(\"accuracy = %f%%\" % accuracy)\n",
    "    print(\"precision = %f\" % precision)     \n",
    "    print(\"recall = %f\" % recall)          \n",
    "    print(\"f1-score = %f\" % f1_score)\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化函数定义，一般使用pytorch默认的初始化方法即可，该模块仅在实验比较不同初始化方法的时候使用\n",
    "\n",
    "import torch\n",
    "\n",
    "def zero(m):\n",
    "\tif hasattr(m, \"weight\"):\n",
    "\t\ttorch.nn.init.zeros_(m.weight.data)\n",
    "\n",
    "def normal(m):\n",
    "\tif hasattr(m, \"weight\"):\n",
    "\t\ttorch.nn.init.normal_(m.weight.data)\n",
    "\n",
    "def orthogonal(m):\n",
    "\tif hasattr(m, \"weight\"):\n",
    "\t\ttorch.nn.init.orthogonal_(m.weight.data)\n",
    "\n",
    "def init(init_type):\n",
    "\tmatch init_type:\n",
    "\t\tcase 1:\n",
    "\t\t\tf = zero\n",
    "\t\tcase 2:\n",
    "\t\t\tf = normal\n",
    "\t\tcase 3:\n",
    "\t\t\tf = orthogonal\n",
    "\t\tcase _ :\n",
    "\t\t\treturn \n",
    "\tfor m in model.children():\n",
    "\t\tif isinstance(m, nn.Conv2d) or isinstance(m, nn.LSTM) or isinstance(m, nn.Linear):\n",
    "\t\t\t\tf(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练新模型————模型选择\n",
    "# 如果要训练新的模型，在此处选择模型；如果只是要验收已训练的模型的结果，不需要执行该模块\n",
    "\n",
    "path =\"tmp.model\"\n",
    "\n",
    "# model = TextCNN()\n",
    "model = TextRNN_LSTM()\n",
    "# model = TextRNN_GRU()\n",
    "# model = TextMLP()\n",
    "\n",
    "# 一般情况下，不需要手动初始化，既不需要执行下面的init函数\n",
    "# init(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练新模型————训练\n",
    "\n",
    "import tqdm\n",
    "import torch.optim as optim\n",
    "\n",
    "train_loader = load_data(\"train\")\n",
    "validation_loader = load_data(\"validation\")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), learning_rate)\n",
    "model.to(device)\n",
    "\n",
    "f1_max = 0\n",
    "for epoch in range(max_epoch):\n",
    "    for data in tqdm.tqdm(train_loader):\n",
    "        sentences, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model.forward(sentences.to(device))\n",
    "        loss = criterion(outputs, labels.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    f1 = evaluate(validation_loader)\n",
    "    if f1 > f1_max:\n",
    "        f1_max = f1\n",
    "        torch.save(model, path)\n",
    "model = torch.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练新模型————测试\n",
    "\n",
    "evaluate(load_data(\"test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 验收模型————即用版\n",
    "# 主要是对前面一些模型和函数定义的整合，如果直接验收的话，不需要管前面的模块，直接先运行本模块，再在下一模块中选择模型类型，在最后一个模块中进行测试即可\n",
    "\n",
    "import json\n",
    "import gensim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 模型参数\n",
    "\n",
    "# 公共参数\n",
    "sentence_len = 50\n",
    "embedding_size = 50\n",
    "batch_size = 50\n",
    "learning_rate = 0.001\n",
    "max_epoch = 10\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format('Dataset/wiki_word2vec_50.bin', binary=True).vectors\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# TextCNN参数\n",
    "kernel_sizes = [3, 5, 7, 9]\n",
    "kernel_nums = 20\n",
    "\n",
    "# TextRNN参数\n",
    "hidden_size = 100\n",
    "hidden_nums = 2\n",
    "\n",
    "# TextMLP参数\n",
    "fc_size = 100\n",
    "\n",
    "# TextCNN模型定义\n",
    "\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(word2vec))\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(in_channels=1, out_channels=kernel_nums, kernel_size=(kernel_size, embedding_size)) for kernel_size in kernel_sizes])\n",
    "        self.full_connect = nn.Linear(in_features=len(kernel_sizes)*kernel_nums, out_features=2)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        conveds = [F.relu(conv(embedded.unsqueeze(1)).squeeze(3)) for conv in self.convs]\n",
    "        pooled = [F.max_pool1d(conv, conv.size(2)).squeeze(2) for conv in conveds]\n",
    "        cat = torch.cat(pooled, dim=1)\n",
    "        full_connected = self.full_connect(cat)\n",
    "        output = self.softmax(full_connected)\n",
    "        return output\n",
    "\n",
    "# TextRNN_LSTM模型定义\n",
    "\n",
    "class TextRNN_LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextRNN_LSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(word2vec))\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers=hidden_nums, bidirectional=True)\n",
    "        self.full_connect = nn.Linear(in_features=2*hidden_size, out_features=2)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        output, (hidden, cell) = self.rnn(embedded.permute(1, 0, 2))\n",
    "        hidden = hidden.view(hidden_nums, 2, -1, hidden_size)\n",
    "        cat = torch.cat((hidden[-1, 0], hidden[-1, 1]), dim=1)\n",
    "        full_connected = self.full_connect(cat)\n",
    "        output = self.softmax(full_connected)\n",
    "        return output\n",
    "\n",
    "# TextRNN_GRU 模型定义\n",
    "\n",
    "class TextRNN_GRU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextRNN_GRU, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(word2vec))\n",
    "        self.rnn = nn.GRU(embedding_size, hidden_size, num_layers=hidden_nums, bidirectional=True)\n",
    "        self.full_connect = nn.Linear(in_features=2*hidden_size, out_features=2)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        output, hidden = self.rnn(embedded.permute(1, 0, 2))\n",
    "        hidden = hidden.view(hidden_nums, 2, -1, hidden_size)\n",
    "        cat = torch.cat((hidden[-1, 0], hidden[-1, 1]), dim=1)\n",
    "        full_connected = self.full_connect(cat)\n",
    "        output = self.softmax(full_connected)\n",
    "        return output\n",
    "\n",
    "# TextMLP模型定义\n",
    "\n",
    "class TextMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextMLP, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(word2vec))\n",
    "        self.full_connect_1 = nn.Linear(in_features=embedding_size*sentence_len, out_features=fc_size)\n",
    "        self.full_connect_2 = nn.Linear(in_features=fc_size, out_features=2)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        full_connected_1 = F.relu(self.full_connect_1(embedded.view(-1, embedding_size*sentence_len)))\n",
    "        full_connected_2 = F.relu(self.full_connect_2(full_connected_1))\n",
    "        output = self.softmax(full_connected_2)\n",
    "        return output\n",
    "\n",
    "# 生成DataLoader的函数定义\n",
    "\n",
    "class EmoDataset(Dataset):\n",
    "    def __init__(self, sentences, labels):\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return torch.tensor(self.sentences[index]), torch.tensor(self.labels[index])\n",
    "\n",
    "def load_data(name):\n",
    "    sentence_file = open(name+\"_sentences.json\")\n",
    "    sentences = json.load(sentence_file)\n",
    "    sentence_file.close()\n",
    "\n",
    "    label_file = open(name+\"_label.json\")\n",
    "    labels = json.load(label_file)\n",
    "    label_file.close()\n",
    "\n",
    "    dataset = EmoDataset(sentences, labels)\n",
    "    return DataLoader(dataset, batch_size, shuffle=True)\n",
    "\n",
    "# 测试与检验的函数定义\n",
    "\n",
    "def evaluate(loader):\n",
    "    P = 0\n",
    "    total = 0\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    T = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            sentences, labels = data\n",
    "            outputs = model.forward(sentences.to(device))\n",
    "            predicts = torch.argmax(outputs, dim=1)\n",
    "            total += len(predicts)\n",
    "            for i in range(len(predicts)):\n",
    "                if predicts[i] == labels[i]:\n",
    "                    T += 1\n",
    "                if predicts[i] == 0:\n",
    "                    if labels[i] == 0:\n",
    "                        TP += 1\n",
    "                    else:\n",
    "                        FP += 1\n",
    "                if labels[i] == 0:\n",
    "                    P += 1\n",
    "    accuracy = float(T) / total * 100\n",
    "    precision = float(TP) / (TP+FP)\n",
    "    recall = float(TP) / P\n",
    "    f1_score = float(2) / (1/precision + 1/recall)\n",
    "    print(\"accuracy = %f%%\" % accuracy)\n",
    "    print(\"precision = %f\" % precision)     \n",
    "    print(\"recall = %f\" % recall)          \n",
    "    print(\"f1-score = %f\" % f1_score)\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 验收旧模型————模型选择\n",
    "\n",
    "# path = \"./TextCNN.model\"\n",
    "# path = \"./TextRNN_LSTM.model\"\n",
    "path = \"./TextRNN_GRU.model\"\n",
    "# path = \"./TextMLP.model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 82.655827%\n",
      "precision = 0.804124\n",
      "recall = 0.857143\n",
      "f1-score = 0.829787\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8297872340425531"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 验收旧模型————测试\n",
    "\n",
    "import torch\n",
    "\n",
    "model = torch.load(path)\n",
    "evaluate(load_data(\"test\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emotion_classify",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
