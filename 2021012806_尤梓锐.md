# 情感分类实验报告

## 模型的结构图与流程分析

### TextCNN

​		结构图如下，流程分析可参照图中文字部分：

​		在从嵌入层到卷积层的过程中为了满足卷积层(batch_size, in_channels, input_length, embedding_dim)的输入要求，需要对嵌入层的输出结果unsqueeze(1)，表示输入通道数为1；卷积输出为(batch_size, kernel_nums, seq_len - filter_size + 1, 1)，因此会需要squeeze(3)把最后一个维度消掉，进行relu。最大池化后的输出为 (batch_size, kernel_nums, 1, 1)，也需要squeeze(2)把后两个维度清除，最后cat得到(batch_size，(卷积核种类数*kernel_nums))的输出

<img src="C:\Users\33044\AppData\Roaming\Typora\typora-user-images\image-20230503160615281.png" alt="image-20230503160615281" style="zoom:50%;" />		在设计时主要参照了课件上的TextCNN结构，与之基本一致，下图是对该模型结构图更形象的表示

<img src="C:\Users\33044\AppData\Roaming\Typora\typora-user-images\image-20230503154249955.png" alt="image-20230503154249955" style="zoom:33%;" />

### TextRNN_LSTM

​		结构图如下，流程分析可参照图中文字部分：

​		从嵌入层到双向LSTM层中，为了满足LSTM的(seq_len, batch_size, embed_size)输入要求，需要对嵌入层（batch_size * sentence_len * embedding_size）的输出进行permute(1, 0, 2)，转为（sentence_len * batch_size * embedding_size）。再从输出的(hidden_nums * 2, batch_size, hidden_size)中通过view(hidden_nums, 2, -1, hidden_size)把前向和后向的hidden_state分离开，(hidden[-1, 0], hidden[-1, 1])把最后一个LSTM层的两个hidden_state拼接交给全连接层。

​		（hidden_nums表示双向LSTM中隐藏层的层数）

<img src="C:\Users\33044\AppData\Roaming\Typora\typora-user-images\image-20230503165833194.png" alt="image-20230503165833194" style="zoom:50%;" />

​		具体双向LSTM层的一个单向结构内部如下，最后利用的是最后一个时间步的两个hidden（正向和反向）输出，即图中的$h_n^{(w)}$：

<img src="C:\Users\33044\AppData\Roaming\Typora\typora-user-images\image-20230503183356350.png" alt="image-20230503183356350" style="zoom:50%;" />

​		而单个LSTM神经元的内部结构可以参照下图：

<img src="C:\Users\33044\AppData\Roaming\Typora\typora-user-images\image-20230503165959230.png" alt="image-20230503165959230" style="zoom:33%;" />

### TextRNN_GRU

​		与LSTM基本一致，只是中间的双向GRU层使用门结构简化了LSTM的结构，参数减少，结构图如下，流程分析可参照图中文字部分，forward的时候对于数据的形状变化等操作也与LSTM中基本一致。

<img src="C:\Users\33044\AppData\Roaming\Typora\typora-user-images\image-20230503173746232.png" alt="image-20230503173746232" style="zoom:50%;" />

​		具体双向GRU层内部结构也与双向LSTM层基本一致，只是没有cell的传递，最后利用的也是最后一个时间步的两个hidden（正向和反向）输出，而单个GRU神经元的内部结构可以参照下图：

<img src="C:\Users\33044\AppData\Roaming\Typora\typora-user-images\image-20230503174430453.png" alt="image-20230503174430453" style="zoom:33%;" />

### TextMLP

​		结构图如下，流程分析可参照图中文字部分：

​		具体嵌入层输入给全连接层1的时候，会把嵌入层输出的后两个维度view成一个改为batch_size * (sentence_len * embeding_size)，这样全连接层1是接收(sentence_len * embeding_size)个输入，输出fc_size

<img src="C:\Users\33044\AppData\Roaming\Typora\typora-user-images\image-20230503175911461.png" alt="image-20230503175911461" style="zoom:50%;" />

## 实验结果

|           |  TextCNN   | TextRNN_LSTM | TextRNN_GRU |  TextMLP   |
| :-------: | :--------: | :----------: | :---------: | :--------: |
| Accuracy  | 81.029810% |  82.655827%  | 83.468835%  | 72.357724% |
| precision |  0.777228  |   0.804124   |  0.807107   |  0.688679  |
|  recall   |  0.862637  |   0.857143   |  0.873626   |  0.802198  |
| F1_score  |  0.817708  |   0.829787   |  0.839050   |  0.741117  |

## 参数比较

​		主要针对TextCNN展开参数分析，其余模型各自选取了部分参数进行测试。另外，因为本身就需要分析参数对拟合效果的影响，因此以下测试中没有为了避免过拟合而使用验证集动态调试，而是直接固定迭代次数进行比较。

​		很重要的一点，因为每次训练时间较长，以下的测试数据并没有多次测试取平均，大多数是只有一次测试的，因此其中很可能有模型参数的随机性的影响。

### TextCNN

		#### 训练迭代次数

|           |  迭代2次   |  迭代5次   |  迭代10次  |  迭代15次  |
| :-------: | :--------: | :--------: | :--------: | :--------: |
| Accuracy  | 77.506775% | 78.861789% | 81.029810% | 76.422764% |
| precision |  0.751269  |  0.750000  |  0.777228  |  0.759563  |
|  recall   |  0.813187  |  0.857143  |  0.862637  |  0.763736  |
| F1_score  |  0.781003  |  0.800000  |  0.817708  |  0.761644  |

​		从中可以看到，起初随着迭代次数的增加，训练效果是逐渐变好的，但也不能一直增加训练迭代次数，否则会出现过拟合的现象，导致最终测试的结果反而下滑。

#### 学习率

|           |     学习率0.1     | 学习率0.01 | 学习率0.001 | 学习率0.0001 |
| :-------: | :---------------: | :--------: | :---------: | :----------: |
| Accuracy  |    50.677507%     | 78.319783% | 81.029810%  |  77.235772%  |
| precision | TP=FP=0，无法计算 |  0.783333  |  0.777228   |   0.731132   |
|  recall   |         0         |  0.774725  |  0.862637   |   0.851648   |
| F1_score  | TP=FP=0，无法计算 |  0.779006  |  0.817708   |   0.786802   |

​		可以看到当一开始学习率非常大的时候几乎是无法成功训练的，因为每次训练方向传播的时候步子迈得太大，最后准确率和随机猜并没有什么区别。而随着准确率减小，学习训练的效果就逐渐显现出来了，准确率逐渐上升。但如果学习率过低也会使得每次训练步伐的调整过小，可能在有限的训练次数中调整不足，从而表现为欠拟合；而当迭代次数极多而学习率极低就更可能出现过拟合的现象。

#### batch_size

|           | batch_size=20 | batch_size=50 | batch_size=500 | batch_size=2000 |
| :-------: | :-----------: | :-----------: | :------------: | :-------------: |
| Accuracy  |  79.403794%   |  81.029810%   |   77.506775%   |   75.338753%    |
| precision |   0.750000    |   0.777228    |    0.743842    |    0.719807     |
|  recall   |   0.873626    |   0.862637    |    0.829670    |    0.818681     |
| F1_score  |   0.807107    |   0.817708    |    0.784416    |    0.766067     |

​		batch_size对训练效果的影响也和前面类似，过小的batch_size可能导致过拟合，而过大的batch_size则有可能导致欠拟合。batch_size更重要的效果是对训练师内存和时间的影响，小的batch_size内存占用小但是训练时间长，大的batc_size训练时间短但是占用内存大，因此对batch_size的选择也需要适中。

#### 卷积核个数

|           | kernel_nums=5 | kernel_nums=20 | kernel_nums=200 | kernel_nums=500 |
| :-------: | :-----------: | :------------: | :-------------: | :-------------: |
| Accuracy  |  79.132791%   |   81.029810%   |   80.758808%    |   81.571816%    |
| precision |   0.758621    |    0.777228    |    0.806630     |    0.816667     |
|  recall   |   0.846154    |    0.862637    |    0.802198     |    0.807692     |
| F1_score  |   0.800000    |    0.817708    |    0.804408     |    0.812155     |

​		在预测的时候，以为实验结果应该也是先上升后下降和之前差不多，但实际上比较意外，即使是增加到了500个卷积核，实验效果也没有出现明显的恶化，从5个到500个，实验的效果都保持地比较好，没有出现预计的因为参数过多过拟合的情况，主要是对于训练时间有明显的影响。

#### 卷积核种数

|           | kernel_size [3, 5] | kernel_size [3, 5, 7, 9] | kernel_size [3, 5, 7, 9, 11, 13] |
| :-------: | :----------------: | :----------------------: | :------------------------------: |
| Accuracy  |     81.029810%     |        81.029810%        |            79.132791%            |
| precision |      0.777228      |         0.777228         |             0.786885             |
|  recall   |      0.862637      |         0.862637         |             0.791209             |
| F1_score  |      0.817708      |         0.817708         |             0.789041             |

​		卷积核种数从实验效果上看其实并不需要太多，光是[3, 5]两个就可以达到比较好的训练效果了，再增加卷积核的种数效果并没有明显改善。个人认为这其实是很合理的，因为大多数时候就是3~5个词可以表达基本的意思，所以用这样大小的卷积核理论上就可以得到基本的训练效果。

### TextRNN

#### 循环层神经元数

|           | hidden_size=20 | hidden_size=100 | hidden_size=500 |
| :-------: | :------------: | :-------------: | :-------------: |
| Accuracy  |   78.590786%   |   82.655827%    |   51.761518%    |
| precision |    0.761421    |    0.804124     |    0.505714     |
|  recall   |    0.824176    |    0.857143     |    0.972527     |
| F1_score  |    0.791557    |    0.829787     |    0.665414     |

​		神经元数目此处便又是很经典的参数过多导致过拟合，训练效果不好的情况，且hidden_size的增加会使得训练时间大大增加，是非常不值当的。

#### 循环层层数

|           | hidden_nums = 1 | hidden_nums = 2 | hidden_nums = 3 |
| :-------: | :-------------: | :-------------: | :-------------: |
| Accuracy  |   81.300813%    |   82.655827%    |   78.048780%    |
| precision |    0.798942     |    0.804124     |    0.806061     |
|  recall   |    0.829670     |    0.857143     |    0.730769     |
| F1_score  |    0.814016     |    0.829787     |    0.766571     |

​		循环层的层数从结果上看也并不需要太多，3层从效果上看应该已经适得其反过拟合了，而且训练的时间也大大增加，因此选择1~2层就应该足够用于本实验了。

### TextMLP

#### 隐藏层层数

|           | fc_sizes = [2] | fc_sizes = [100, 2] | fc_sizes = [500, 100, 2] |
| :-------: | :------------: | :-----------------: | :----------------------: |
| Accuracy  |   72.899729%   |     72.357724%      |        69.647696%        |
| precision |    0.711340    |      0.759563       |         0.634615         |
|  recall   |    0.758242    |      0.802198       |         0.906593         |
| F1_score  |    0.734043    |      0.761644       |         0.746606         |

​		MLP的全连接层数的影响也同样满足太小欠拟合、太大过拟合的性质，但我在实验中观察到最明显的是全连接层数对MLP稳定性的影响，当层数仅有一层的时候，实验效果的随机性非常明显，时常F1_score会只有0.6上下；而在层数为2的时候，稳定性稍微有好转，F1_score极低的情况发生比较少；最后在层数为3的时候，F1_score极低的情况就几乎不会发生。个人认为应该是参数少的时候受参数初始化时的随机性影响比较大所以需要增加全连接层层数来提高稳定性。

## baseline比较

​		以TextMLP作为baseline进行比较，可以看到另外三个模型的Accuracy和F1_score都远高于TextMLP；但是MLP在训练速度上会远比另外三者来得快；而至于稳定性MLP则又是远远不如另二者。

### 速度

​		实际上MLP模型中应该具有更多的参数，但其中的计算都相对比较简单，基本都是线性，加上relu、softmax而已，因此再反向传播的时候计算梯度会比较简单，BP算法跑起来也就比较快；而CNN、LSTM、GRU中则有更多更为复杂的计算，因此反向传播的时候会相对慢一些。

### 准确度

​		我个人认为MLP训练效果远不如CNN和RNN的原因在于实际上MLP中没有识别一个词的概念，是把50个词向量的50个分量一共2500个数据列在一起作为输入，又是全连接，那么其实不存在词的概念，所以训练效果会很不好。而在CNN和RNN中，都明显用到了词的概念：CNN中进行卷积运算的时候，每次是对几个词向量整体做卷积，没有将一个词向量拆分；在RNN中每个时间步都对应了一个词向量的输入，也是以词为输入单位，没有把一个词向量拆分。

### 稳定性

​		稳定性主要体现在每次面对随机的初始化参数，模型的表现是否稳定，有多大的概率会出现训练效果远低于一般情况的现象。在我个人的实验过程中，MLP的稳定性是比较差的，时不时会出现F1_score在0.6上下的情况，F1_score的变化幅度过大。其他的三个模型虽然每次的训练效果也有波动，但变化较小，基本上F1_score的变化范围不会超过±0.3，且出现较低的F1_score的概率也都低于MLP。

## 问题思考

###  Q1

​		理论上训练在loss最小的时候停止是最合适的，可以尝试比较每次的loss相对上一次训练是否减小，如果没有减小则停止训练，这个方法虽然不能保证绝对将loss训练到最小，但在一定程度上有帮助。

​		因为这次作业主要的验收指标是f1_score，所以我的最初的实现方法是在每轮迭代后在验证集上计算f1_score，下面是最开始的决定迭代轮数的方法：如果f1_score比之前高了，那么就保存这个模型，并继续训练；反之就停止迭代，从保存的模型中load替换掉这次训练的结果。另外设置了一个训练迭代的次数上限max_epoch。具体可以参看下面的伪代码：

```python
f1_max = 0
for epoch in range(max_epoch):
   	train()
    f1 = evaluate()
    if f1 < f1_max:
        model = torch.load(path)
        break
    else:
        f1_max = f1
        torch.save(model, path)
```

​		固定迭代次数如果次数过多可能会使得训练过度出现过拟合，如果次数过少可能模型的训练不充分，这个固定的值是不好确定的，而且由于训练存在随机性，本身这个最佳的迭代次数就可能不是固定的，所以从训练效果上，固定迭代次数往往不是一个理想的方案。

​		而通过验证集动态调整训练迭代次数在效果上应该会更好，当训练结果的指标开始下降的时候及时结束训练可以在一定程度上避免过拟合与训练不足的问题，但同样也无法完全保证训练达到了最佳的效果。另外由于训练的过程中加入了验证的步骤，所以训练的速度也会有所减慢。除此之外这个方法的明显缺陷在于如果某次训练因为随机性等原因使得指标下降了，训练会就此终止，但可能此时训练并未充分，该方法的鲁棒性是远不如固定迭代次数的。

​		针对二者的优缺点，我最后将两种判定训练是否终止的方法进行了结合，即训练固定迭代次数，避免偶然性导致训练提前终止，但只在f1_score上升的迭代后保存这次迭代的模型，具体可以参考下面的伪代码：

```python
f1_max = 0
for epoch in range(epoch_nums):
    train()
    f1 = evaluate()
    if f1 > f1_max:
        f1_max = f1
        torch.save(model, path)
model = torch.load(path)
```

​		本来想尝试如果f1_score下降了就倒回到上一次训练的结果，但这样会导致无法继续训练，模型卡住不动。

### Q2

​		TextCNN和TextRNN_LSTM模型上对不同的初始化方法进行了实验对比，每次实验中对全连接层、卷积层、LSTM层使用统一的测试初始化方法。

|  TextCNN  | 默认初始化 | 零均值初始化 | 高斯分布初始化 | 正交初始化 |
| :-------: | :--------: | :----------: | :------------: | :--------: |
| Accuracy  | 81.029810% |  65.853659%  |   49.322493%   | 73.441734% |
| precision |  0.777228  |   0.607692   |    0.493225    |  0.664062  |
|  recall   |  0.862637  |   0.868132   |    1.000000    |  0.934066  |
| f1_score  |  0.817708  |   0.714932   |    0.660617    |  0.776256  |

| TextRNN_LSTM | 默认初始化 | 零均值初始化 | 高斯分布初始化 | 正交初始化 |
| :----------: | :--------: | :----------: | :------------: | :--------: |
|   Accuracy   | 82.655827% |  80.487805%  |   76.151762%   | 80.216802% |
|  precision   |  0.804124  |   0.792553   |    0.700855    |  0.753488  |
|    recall    |  0.857143  |   0.818681   |    0.901099    |  0.890110  |
|   f1_score   |  0.829787  |   0.805405   |    0.788462    |  0.816121  |

​		在TextCNN模型中，换用另外3种初始化方法，导致最后的训练结果明显的差于默认初始化方法，且不同初始化方法的训练结果差异较大；而在TextRNN_LSTM模型中，换用另外3种初始化方法得到的训练结果和默认初始化都比较接近，而且不同方法之间的差距也比较小，甚至之间的差距不能保证是初始化方法差异引起的，而可能是训练过程中的随机和偶然性的结果。数据上共性的一点应该是训练效果默认方法 > 正交初始化 > 零均值初始化 > 高斯分布初始化。

​		查找了有关资料，对于三种初始化方法的归纳大致如下：

​		零均值初始化方法将所有参数初始化为0，这种方法理论上训练效果应该不好，因为会导致开始时所有神经元的输出相同，并且许多处的导数为0，但是在实际训练中效果比预料的要好一些。

​		高斯分布初始化使用高斯分布随机初始化权重参数，但在方差过大的地方可能会出现导致梯度消失或梯度爆炸的问题，实际上的训练效果比预料的要差许多。

​		正交初始化使用正交矩阵来初始化权重参数，确保每个神经元输出的方差相等，适用于卷积神经网络和循环神经网络，这一点在实际训练过程中也得到了证实，确实训练的效果要优于另外两种初始化方法。

​		比起不同初始化方法之间的差异，这个实验的意义我觉得更在于发现了CNN和RNN对于不同初始化方法的鲁棒性的差异，RNN方法在这方面显著地优于CNN，这也可以作为RNN的一大优点。

### Q3

​		1、通过验证集实时校验训练结果，发现训练指标下降时及时终止，避免因为训练迭代次数过多引起的过拟合。

​		2、可以通过引入正则化项的方式，限制模型的复杂度，从而避免过拟合。

​		3、可以引入dropout方法，在训练过程中每次随机舍弃一些神经元，减少参数，避免过拟合。

​		4、可以通过数据增强的方式，对训练数据进行一些随机的变换，例如旋转、翻转、剪裁等，使得模型可以学到更多的特征，避免过度拟合。

### Q4

​		CNN模型每次读取局部的几个词向量，可以很好地识别数据的局部特征；同时卷积核中的参数共享，也就使得参数量比一般的模型要少，训练起来速度更快；整体上看，CNN模型对于图像、视频等空间结构化的数据类型处理起来比较有优势。

​		RNN模型的特点在于记忆，在生成当前时间步的输出时采纳了上一个时间步的内容，双向RNN从两个方向同时进行了学习，LSTM避免了较远的学习内容被遗忘，因此整体上RNN的记忆效果比较突出，适合处理文字、语音等序列化的数据，但整体训练的速度比较慢。

​		MLP模型相对比较简单，虽然参数比较多，但训练迭代的速度比较快，对于一些简单的分类、回归问题有比较好的处理效果，缺陷在于模型较为简单，且参数较多，容易出现过拟合，并且不适合处理一系列具有局部特征的问题。

## 心得体会

​		在这次的实验过程中主要是实现了TextCNN、TextRNN_LSTM、TextRNN_GRU和TextMLP四个深度学习的模型，比较了不同模型、不同参数、训练终止时机、不同初始化方法等等对训练学习效果的影响。

​		在实现模型的过程中其实最困难的是在层与层之间对数据形状的变化，初学的过程中，squeeze、view、cat等方法的运用并不熟练，实际上更多是参考网上许多开源项目去理解每一步为什么要调整改变数据的形状。

​		从训练时间上，MLP快于CNN，CNN快于RNN，理论上RNN中GRU应该快于LSTM，但实际实验过程中我的GRU模型要比LSTM慢很多，也是因此最后分析参数的时候都选择了RNN中都选择了LSTM没有使用GRU。从训练效果上，CNN、LSTM、GRU都取得了比较不错的训练效果，并且结果比较相近，而MLP的效果则不尽人意，在前文中也分析了应该是我的MLP模型没有利用到词的概念所导致的。从稳定性上，CNN、LSTM、GRU的稳定性都还不错，而MLP的稳定性则比较差，F1_score不仅经常出现低值而且与一般情况的差值较大，但在对MLP的全连接层层数进行实验的过程中发现，通过增加全连接层层数的方法可以有效地提高MLP模型的稳定性。

​		在参数分析的过程中，多数参数其实保持一个共同的特点，给太少了，模型欠拟合，效果不好；给太多了，模型过拟合，效果也不好。因此对于实验的绝大多数参数如迭代次数、学习率、batch_size、卷积核种数等等，都需要找一个适中的值。而对于另一些参数比如卷积核种数，并没有出现预计的因为给的过多而出现过拟合的现象，改变该参数对模型的影响也不大，对这些参数的选择可以开放。

​		训练终止时机方面最后在固定迭代次数与利用验证集调整的思路下受到启发，将二者相结合，确定了最后的方案：每次迭代如果效果优于之前的就保存，否则不保存继续迭代，迭代固定的次数，最后加载保存的模型。

​		在对不同初始化方法的研究过程中，除了发现不同的初始化方法对训练效果的不同影响之外，更意外的是发现了不同的模型对于初始化方法的变动，训练效果的变动也十分不同。CNN在不同的初始化方法下的训练效果差异很大，而RNN则并不明显，在一定程度上表现RNN的稳定性应该优于CNN。至于不同的初始化方法，虽然在我的实验中貌似都不及pytorch自己的默认初始化，但整体上正交初始化优于零均值初始化，再优于高斯分布初始化，令人意外的是零均值居然会优于高斯分布，应该是高斯分布初始化没有展现出理论上那么优秀的效果。